{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear and reset the workspace\n",
    "%reset -f\n",
    "\n",
    "### Import necessary libraries \n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import xlwt \n",
    "from xlwt import Workbook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the labels\n",
    "def classify(FREQUENCY, classes, all_available_data_df):\n",
    "    \n",
    "    target = []\n",
    "    perc_difference = (all_available_data_df['Close'].shift(-FUTURE_PERIOD_PREDICT) - all_available_data_df['Close']) / all_available_data_df['Close'] * 100\n",
    "\n",
    "    if FREQUENCY == '1Min':\n",
    "        boundary_down = [-100 , -0.60, -0.45, -0.30, -0.15, 0.00, 0.15, 0.30, 0.45, 0.60]\n",
    "        boundary_up =   [-0.60, -0.45, -0.30, -0.15,  0.00, 0.15, 0.30, 0.45, 0.60, 100]\n",
    "    \n",
    "    for perc_diff in perc_difference:\n",
    "        if perc_diff > boundary_down[0] and perc_diff <= boundary_up[0]:\n",
    "            target.append(classes[0]) \n",
    "        elif perc_diff > boundary_down[1] and perc_diff <= boundary_up[1]:\n",
    "            target.append(classes[1])\n",
    "        elif perc_diff > boundary_down[2] and perc_diff <= boundary_up[2]:\n",
    "            target.append(classes[2])\n",
    "        elif perc_diff > boundary_down[3] and perc_diff <= boundary_up[3]:\n",
    "            target.append(classes[3])\n",
    "        elif perc_diff > boundary_down[4] and perc_diff <= boundary_up[4]:\n",
    "            target.append(classes[4])\n",
    "        elif perc_diff > boundary_down[5] and perc_diff <= boundary_up[5]:\n",
    "            target.append(classes[5])\n",
    "        elif perc_diff > boundary_down[6] and perc_diff <= boundary_up[6]:\n",
    "            target.append(classes[6])\n",
    "        elif perc_diff > boundary_down[7] and perc_diff <= boundary_up[7]:\n",
    "            target.append(classes[7])\n",
    "        elif perc_diff > boundary_down[8] and perc_diff <= boundary_up[8]:\n",
    "            target.append(classes[8])\n",
    "        elif perc_diff > boundary_down[9] and perc_diff <= boundary_up[9]:\n",
    "            target.append(classes[9])\n",
    "            \n",
    "    all_available_data_df['Target'] = pd.DataFrame(target)\n",
    "    all_available_data_df.dropna(inplace=True)\n",
    "    \n",
    "    return all_available_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the learning\n",
    "def preprocess_df(df, SEQ_LEN, classes):\n",
    "    \n",
    "    for col in df.columns:  # go through all of the columns\n",
    "        if col != \"Target\":  # normalize all ... except for the target itself!\n",
    "            df[col] = (df[col]-df[col].mean()) / df[col].std()\n",
    "            #df[col] = df[col] / (df[col].max()-df[col].min())\n",
    "            df.dropna(inplace=True) \n",
    "            \n",
    "    df.dropna(inplace=True) \n",
    "    \n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # \n",
    "            sequential_data.append([np.array(prev_days), np.eye(len(classes))[int(i[-1])]])  # append those bad boys!\n",
    "    \n",
    "    random.shuffle(sequential_data)  \n",
    "    \n",
    "    sequenced_classes = []  # list that will store our  sequences and targets\n",
    "    for i in range(len(classes)):\n",
    "        sequenced_classes.append([])\n",
    "    \n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        sequenced_classes[np.argmax(target)].append([seq, target])  \n",
    "     \n",
    "    all_sequential_data = []\n",
    "    lower = df['Target'].value_counts().min() # Balance data w.r.t a class with a minimum # of sample \n",
    "    for i in range(len(classes)):\n",
    "        random.shuffle(sequenced_classes[i])\n",
    "        sequenced_classes[i] = sequenced_classes[i][:lower]\n",
    "        all_sequential_data = all_sequential_data + sequenced_classes[i]\n",
    "    random.shuffle(all_sequential_data)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "        \n",
    "    for seq, target in all_sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels \n",
    "    \n",
    "        \n",
    "    return np.array(X),np.array(y) #np.array(y_last)  # return X and y...and make X a numpy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the Neural Net\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, IMG_HEIGHT, IMG_WIDTH, output_size):\n",
    "        '''\n",
    "        First inherit the nn.module class to use pytorch.\n",
    "        Then add the convulutional layers.\n",
    "        Do the flattening.\n",
    "        Then liner/dense layerse can be added accordingly.\n",
    "        '''\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        \n",
    "        self.IMG_HEIGHT = IMG_HEIGHT\n",
    "        self.IMG_WIDTH  = IMG_WIDTH\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=IMG_HEIGHT-IMG_WIDTH) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 3x3 kernel / window\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "   \n",
    "        x = torch.randn(self.IMG_HEIGHT,self.IMG_WIDTH).view(-1,1,self.IMG_HEIGHT,self.IMG_WIDTH) # A random tensor is passed through the conv. layers once so that\n",
    "        self._to_linear = None                  # the size of the output of the last conv. layer can be found.\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 512) # \n",
    "        self.fc3 = nn.Linear(512, 512) # \n",
    "        self.fc4 = nn.Linear(512, 512) # \n",
    "        self.fc5 = nn.Linear(512, 512) #\n",
    "        self.fc6 = nn.Linear(512, self.output_size) #\n",
    "        \n",
    "\n",
    "    def convs(self, x):\n",
    "        '''\n",
    "        Applying max_pooling and using appropiate activation function for conv. layers\n",
    "        '''\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Reshaping after conv. layers accordingly \n",
    "        Then applying activation functions to the linear layers and the output layers.\n",
    "        '''\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.dropout(x, p=0.35, training=True, inplace=False)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x) # bc this is our output layer. No activation here.\n",
    "        \n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, MODEL_NAME,  BATCH_SIZE, EPOCHS, test_X, test_y, testing_size, IMG_HEIGHT, IMG_WIDTH):\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "        self.IMG_HEIGHT = IMG_HEIGHT\n",
    "        self.IMG_WIDTH  = IMG_WIDTH\n",
    "        self.size = testing_size        \n",
    "        self.MODEL_NAME = MODEL_NAME\n",
    "    \n",
    "    def fwd_pass(self, X, y, train=False):\n",
    "        '''\n",
    "        Passing the data for both training and testing\n",
    "        '''\n",
    "        if train:\n",
    "            net.zero_grad()\n",
    "        outputs = net(X)\n",
    "        matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "        acc = matches.count(True)/len(matches)\n",
    "        loss = loss_function(outputs, y)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return acc, loss\n",
    "    \n",
    "    \n",
    "    def fit(self, net, train_X, train_y):\n",
    "        '''\n",
    "        Train the data, set HYPER_PARAMETERS(Epocsh, batch_size).\n",
    "        Set the optimizer    \n",
    "        '''\n",
    "        wb = Workbook()\n",
    "        sheet_name = self.MODEL_NAME \n",
    "        s1 = wb.add_sheet(sheet_name)\n",
    "        s1.write(0,0,'Training Accuracy')\n",
    "        s1.write(0,1,'Test Accuracy')\n",
    "        s1.write(0,2,'Training Loss')\n",
    "        s1.write(0,3,'Test Loss')\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), self.BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+self.BATCH_SIZE].view(-1, 1, IMG_HEIGHT, IMG_WIDTH)\n",
    "                batch_y = train_y[i:i+self.BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = self.fwd_pass(batch_X, batch_y, train=True)\n",
    "                \n",
    "                if i == int(len(train_X)/self.BATCH_SIZE) * self.BATCH_SIZE /2 :\n",
    "                    random_start = np.random.randint(len(self.test_X)-self.size)\n",
    "                    X, y = self.test_X[random_start:random_start+self.size], self.test_y[random_start:random_start+self.size]\n",
    "                    with torch.no_grad():\n",
    "                        val_acc, val_loss = self.fwd_pass(X.view(-1, 1, IMG_HEIGHT, IMG_WIDTH).to(device), y.to(device))\n",
    "                    print(f\"Epoch  {epoch+1} :\\n\")\n",
    "                    print(' Training Accuracy :', acc, '\\n', 'Test Accuracy :', val_acc, '\\n', 'Training Loss :', loss.item(), '\\n', 'Test Loss :', val_loss.item())   \n",
    "                    s1.write(epoch+1,0, round(float(acc),3))\n",
    "                    s1.write(epoch+1,1, round(float(val_acc),3))\n",
    "                    s1.write(epoch+1,2, round(float(loss), 4))\n",
    "                    s1.write(epoch+1,3, round(float(val_loss), 4))\n",
    "        filename = self.MODEL_NAME +'.xls'\n",
    "        wb.save(filename) \n",
    "            \n",
    "            \n",
    "    def predict(self, net, X_pred):\n",
    "        X_pred = X_pred.view(-1, 1, IMG_HEIGHT, IMG_WIDTH)\n",
    "        X_pred = X_pred.to(device)\n",
    "        outputs = net(X_pred)\n",
    "        prediction = torch.argmax(outputs)\n",
    "        if prediction == 0:\n",
    "            print('It is a cat with probability of ',outputs[0])\n",
    "        else:\n",
    "            print('It is a dog with probability of ',outputs[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main Code ###\n",
    "# Loading the data, if data is already prepared once, just load it. Once the data is prepared, save it and turn the flag to False\n",
    "REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "X_name = \"X_all.npy\"\n",
    "y_name = \"y_all.npy\"\n",
    "\n",
    "# Set the preprocessing HYPERPARAMETERS\n",
    "FREQUENCY = '1Min'\n",
    "SEQ_LEN =   60 #how long of a preceeding sequence to collect for R\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "classes   = [0,1,2,3,4,5,6,7,8,9]\n",
    "columns_to_load = [1,2,3,4,5]\n",
    "IMG_HEIGHT = SEQ_LEN\n",
    "IMG_WIDTH = len(columns_to_load)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    # Load the data\n",
    "    file = 'TSLA.USUSD_Candlestick_1_M_BID_24.07.2017-24.07.2020.csv'\n",
    "    all_available_data_df = pd.read_csv(file, usecols=columns_to_load) \n",
    "\n",
    "    all_availabl_df = classify(FREQUENCY, classes, all_available_data_df)\n",
    "    X_all, y_all = preprocess_df(all_availabl_df, SEQ_LEN, classes)\n",
    "    \n",
    "    np.save(\"X_all.npy\", X_all)\n",
    "    np.save(\"y_all.npy\", y_all)\n",
    "else:\n",
    "    X_all = np.load(X_name, allow_pickle=True)\n",
    "    y_all = np.load(y_name, allow_pickle=True)\n",
    "\n",
    "### Define a device to start GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "    \n",
    "### Connecting the Neural Net to the GPU. Choosing the loss function, and optimizer parameters\n",
    "LEARNING_RATE = 0.001\n",
    "net = Net(IMG_HEIGHT, IMG_WIDTH, len(classes)).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "print('Neural Net Features : ',net)\n",
    "print('Optimizer Features : ',optimizer)\n",
    "print('Loss Function : ', loss_function)\n",
    "\n",
    "### Splitting the data as training and testing data\n",
    "VAL_PCT = 0.1  # Test set ratio\n",
    "val_size = int(len(X_all)*VAL_PCT)\n",
    "\n",
    "X_all = torch.Tensor([i for i in X_all]).view(-1,IMG_HEIGHT, IMG_WIDTH)\n",
    "y_all = torch.Tensor([i for i in y_all])\n",
    "\n",
    "train_X = X_all[:-val_size]\n",
    "train_y = y_all[:-val_size]\n",
    "\n",
    "test_X = X_all[-val_size:]\n",
    "test_y = y_all[-val_size:]\n",
    "\n",
    "print('Size of the Data Set :', len(X_all))\n",
    "print('Training Set Ratio [%] :', 100 * VAL_PCT)\n",
    "print('Size of the Train Set :', len(train_X), ' <---> Size of the Test Set :', len(test_X))\n",
    "\n",
    "### Training HYPERPARAMETERS\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "testing_size = 300 # len(test_X) -1 #\n",
    "\n",
    "batch = BATCH_SIZE\n",
    "MODEL_NAME = f\"stocks_v3_model-Batch{int(batch)}\"\n",
    "\n",
    "print('\\n HYPERPARAMETERS : ')\n",
    "print('Batch Size : ', batch)\n",
    "print('# of Epochs : ', EPOCHS)\n",
    "\n",
    "model = Model(MODEL_NAME, batch, EPOCHS, test_X, test_y, testing_size, IMG_HEIGHT, IMG_WIDTH)\n",
    "model.fit(net, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data Visualization #####\n",
    "columns = ['Training Accuracy', 'Test Accuracy', 'Training Loss', 'Test Loss' ]\n",
    "results_df = pd.read_excel(MODEL_NAME+'.xls')\n",
    "results_df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['Training Accuracy','Test Accuracy']].plot(grid=True)\n",
    "results_df[['Training Loss','Test Loss']].plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
